{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Create a daily-updated data archive of observed meteorology:\n",
    "        Stakeholders are Salient's Machine Learning team and our customers\n",
    "        Duration limit to complete the task is a 2 hour timeframe, enforced on the honor system\n",
    "        Deadline to submit an answer is 1 week after receipt of this email\n",
    "    - For now, the archive will contain 3 different observed met station WBAN codes:\n",
    "        14739 (Boston), 23169 (Las Vegas), 94846 (Chicago)\n",
    "        Eventually, this system must scale to handle all >100k GHCNd stations\n",
    "    - Get data from NCEI, example for Boston:\n",
    "        https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/USW000014739.csv\n",
    "    - Output is a zarr archive:\n",
    "        Coordinates:   ghcn_id & time, chunked at your discretion\n",
    "        Data variables: precip (mm/day),  tmax (째C), tmin (째C)\n",
    "            The source data calls it \"prcp\", so you'll have to change it\n",
    "    - Write functions:\n",
    "        build_ghcnd_archive that establishes a fresh archive from scratch\n",
    "        update_ghcnd_archive that updates the archive each day\n",
    "    - Answer questions with 1-3 sentences:\n",
    "        How would you orchestrate this system to run at scale?\n",
    "        What major risks would this system face?\n",
    "        What are the next set of enhancements you would add?\n",
    "        How would you improve the clarity of this assignment?\n",
    "    - Reply here to send your answer as zipped (.py | .ipynb) & .pdf\n",
    "        PDF must contain a print statement that shows the archive contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = (\"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/{station}.csv\")\n",
    "\n",
    "stations = ['USW00014739', 'USW00023169', 'USW00094846']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ghcnd_https_download(station_id):\n",
    "    \n",
    "    base_url = (\"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/{station}.csv\")\n",
    "    \n",
    "    \n",
    "    url = base_url.format(station = station_id)\n",
    "    try:\n",
    "        raw_data = requests.get(url, timeout=2, stream=True)\n",
    "        #Check for http connectivity\n",
    "        raw_data.raise_for_status() \n",
    "        \n",
    "        raw_data_frame = pd.read_csv(io.StringIO(raw_data.text), parse_dates=['DATE'])\n",
    "        \n",
    "        df = data_transform(raw_data_frame)\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        \n",
    "        return f\"Failed to download {station_id}: {str(e)}\"\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(data_frame):\n",
    "    df = data_frame.set_index(['STATION', 'DATE'])\n",
    "        \n",
    "    df = df.rename(columns={'PRCP': 'precip', 'TMAX': 'tmax', 'TMIN': 'tmin'})\n",
    "    \n",
    "    \n",
    "    df['precip'] = df['precip'] / 10  # Convert tenths to mm\n",
    "    df['tmax'] = df['tmax'] / 10  # Convert to 째C\n",
    "    df['tmin'] = df['tmin'] / 10  # Convert to 째C\n",
    "    \n",
    "    df = df.loc[:, ['precip', 'tmax', 'tmin']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(stations):\n",
    "    results = []\n",
    "    #Download multiple files \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        tasks = {executor.submit(ghcnd_https_download, station): station for station in stations}\n",
    "        for future in as_completed(tasks):\n",
    "            output = tasks[future]\n",
    "            try:\n",
    "                #Get the results from each task for further processing\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print('error with '+output)\n",
    "                \n",
    "            #results.append(f\"Unexpected error downloading {}: {str(e)}\")   \n",
    "    combined_data = pd.concat(results)\n",
    "    ds = combined_data.to_xarray()\n",
    "    ds = ds.rename({'STATION': 'ghcnd_id', 'DATE': 'time'})\n",
    "\n",
    "    # Chunk the data (adjust as needed)\n",
    "    ds = ds.chunk({'ghcnd_id': 1, 'time': -1})\n",
    "    \n",
    "    return(ds)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ghcnd_archive(output_path, stations):\n",
    "    \n",
    "    all_stations_ds = download_files(stations)\n",
    "    \n",
    "    all_stations_ds.to_zarr(output_path, mode='w')\n",
    "    return all_stations_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m4/3slgbrxj3z3dm65l82539j6w0000gq/T/ipykernel_98543/4177091645.py:12: DtypeWarning: Columns (17,19,21,23,25,27,29,31,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,81,87,89,91,93,95,97,99,101,103,105,107,109,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data_frame = pd.read_csv(io.StringIO(raw_data.text), parse_dates=['DATE'])\n",
      "/var/folders/m4/3slgbrxj3z3dm65l82539j6w0000gq/T/ipykernel_98543/4177091645.py:12: DtypeWarning: Columns (17,19,21,23,25,27,29,31,33,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,77,85,87,89,91,93,95,97,99,101,103) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data_frame = pd.read_csv(io.StringIO(raw_data.text), parse_dates=['DATE'])\n",
      "/var/folders/m4/3slgbrxj3z3dm65l82539j6w0000gq/T/ipykernel_98543/4177091645.py:12: DtypeWarning: Columns (17,19,21,23,25,27,29,31,33,37,39,41,43,45,47,49,51,53,55,57,61,63,65,67,69,73,83,89,91,93,95,97,101,105,107,109) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data_frame = pd.read_csv(io.StringIO(raw_data.text), parse_dates=['DATE'])\n"
     ]
    }
   ],
   "source": [
    "archive = build_ghcnd_archive('archive.zarr', stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ghcnd_archive(archive_path, stations):\n",
    "    todays_date = pd.Timestamp.today()\n",
    "    try:\n",
    "        ds = xr.open_zarr(archive_path)\n",
    "    except FileNotFoundError:\n",
    "        print('No archive created')\n",
    "    \n",
    "    for station_id in stations:\n",
    "        station_da = ds.sel(ghcnd_id = station_id)\n",
    "    \n",
    "        last_station_date = station_da['time'].values[-1]\n",
    "        days_outdated = (todays_date - last_station_date).days\n",
    "        \n",
    "        if days_outdated > 0:\n",
    "            new_da = ghcnd_https_download(station_id)\n",
    "            \n",
    "            \n",
    "            new_ds = new_da.to_xarray()\n",
    "            \n",
    "            new_ds = new_ds.rename({'STATION': 'ghcnd_id', 'DATE': 'time'})\n",
    "    \n",
    "            new_ds = new_ds.chunk({'ghcnd_id': 1, 'time': -1})\n",
    "            \n",
    "            ds = xr.merge([ds, new_ds], compat='override')\n",
    "    \n",
    "            # Write the updated dataset back to the zarr store\n",
    "    ds.to_zarr(archive_path, mode='a')     \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m4/3slgbrxj3z3dm65l82539j6w0000gq/T/ipykernel_98543/4177091645.py:12: DtypeWarning: Columns (17,19,21,23,25,27,29,31,33,37,39,41,43,45,47,49,51,53,55,57,61,63,65,67,69,73,83,89,91,93,95,97,101,105,107,109) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data_frame = pd.read_csv(io.StringIO(raw_data.text), parse_dates=['DATE'])\n",
      "/var/folders/m4/3slgbrxj3z3dm65l82539j6w0000gq/T/ipykernel_98543/4177091645.py:12: DtypeWarning: Columns (17,19,21,23,25,27,29,31,33,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,77,85,87,89,91,93,95,97,99,101,103) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data_frame = pd.read_csv(io.StringIO(raw_data.text), parse_dates=['DATE'])\n",
      "/var/folders/m4/3slgbrxj3z3dm65l82539j6w0000gq/T/ipykernel_98543/4177091645.py:12: DtypeWarning: Columns (17,19,21,23,25,27,29,31,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,81,87,89,91,93,95,97,99,101,103,105,107,109,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data_frame = pd.read_csv(io.StringIO(raw_data.text), parse_dates=['DATE'])\n"
     ]
    }
   ],
   "source": [
    "update_ghcnd_archive('archive.zarr', stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 3MB\n",
      "Dimensions:   (ghcnd_id: 3, time: 32364)\n",
      "Coordinates:\n",
      "  * ghcnd_id  (ghcnd_id) object 24B 'USW00014739' 'USW00023169' 'USW00094846'\n",
      "  * time      (time) datetime64[ns] 259kB 1936-01-01 1936-01-02 ... 2024-08-09\n",
      "Data variables:\n",
      "    precip    (ghcnd_id, time) float64 777kB dask.array<chunksize=(1, 32364), meta=np.ndarray>\n",
      "    tmax      (ghcnd_id, time) float64 777kB dask.array<chunksize=(1, 32364), meta=np.ndarray>\n",
      "    tmin      (ghcnd_id, time) float64 777kB dask.array<chunksize=(1, 32364), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How would you orchestrate this system to run at scale?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would run on a more powerful system, such as AWS or Google Cloud, that would be able to download more files at once to actually create the initial archive, and then save to an S3 store. To update the archive, there are computationally expensive operations that take place. To avoid this expense, we would want to try and only download the newest parts of the file, which should be possible using a CSV format since it's just a text based format. I would use a scheduler and some form of dashboard to make sure all 100K stations download without issue, and are updated everyday. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What major risks would this system face?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As written, this system is susceptible to changes in URL formats, CSV metadata, or other changes that impact how the data is structured. If these components change, the system will fail because there is not extensive error checking. The system would also break if there are network connectivity issues and the URL is unreachable. By adding a timeout to the GET request, I mitigate this a bit, but not gracefully. Finally, we do not do any quality control checking, so errors in the actual data sourced from NCEI would pose problems for further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the next set of enhancements you would add?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would add significantly more error checking to the code to make sure that if the previously mentioned risks occur, that they don't cause silent failures. I would add QA/QC before building the archive to make sure we aren't poisoning our ML/AI ops with bad data. Finally, I would try to further improve performance by finding more efficient ways of updating the archive, adjust the chunking and compression structure of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How would you improve the clarity of this assignment?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was tripped up by the daily updating function. To update something daily, I mostly use cron which falls outside the scope of the assignment. It might be clearer to say \"write a function that if run every day, will update the archive with new data\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salient-interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
